{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adamMcneil/hot-topics-data-management-project/blob/main/598_mp4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHOxFKTMPPFV",
        "outputId": "8750251a-bc50-429a-adc9-2101cd5efbd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.13.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting scann\n",
            "  Downloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from scann) (5.29.4)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scann-1.4.0-cp311-cp311-manylinux_2_27_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scann, faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0 scann-1.4.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install h5py faiss-cpu requests scann\n",
        "import faiss\n",
        "import h5py\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import requests\n",
        "import scann\n",
        "\n",
        "SIFT1M_URL = \"http://ann-benchmarks.com/sift-128-euclidean.hdf5\"\n",
        "SIFT1M_FILENAME = \"sift-128-euclidean.hdf5\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qa_yzdxePPFW"
      },
      "outputs": [],
      "source": [
        "def download_sift1m():\n",
        "    \"\"\"Downloads the SIFT1M dataset if it's not already present.\"\"\"\n",
        "    if not os.path.exists(SIFT1M_FILENAME):\n",
        "        print(\"Downloading SIFT1M dataset...\")\n",
        "        response = requests.get(SIFT1M_URL, stream=True)\n",
        "        with open(SIFT1M_FILENAME, \"wb\") as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "        print(\"Download complete.\")\n",
        "    else:\n",
        "        print(\"SIFT1M dataset already exists.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "94SAPcExPPFW"
      },
      "outputs": [],
      "source": [
        "def load_sift1m():\n",
        "    with h5py.File(\"sift-128-euclidean.hdf5\", \"r\") as f:\n",
        "        train_data = np.array(f[\"train\"], dtype=np.float32)\n",
        "        test_queries = np.array(f[\"test\"], dtype=np.float32)\n",
        "        ground_truth = np.array(f[\"neighbors\"], dtype=np.int64)[:, 0]\n",
        "    print(\"Dataset loaded.\")\n",
        "    return train_data, test_queries, ground_truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uxlEyIxwPPFW"
      },
      "outputs": [],
      "source": [
        "def evaluate_hnsw(train_data, test_queries, ground_truth, M=32, efSearch_vals=[10, 50, 100, 200]):\n",
        "    d = train_data.shape[1]\n",
        "    index = faiss.IndexHNSWFlat(d, M, faiss.METRIC_L2)\n",
        "    index.hnsw.efConstruction = 200\n",
        "    index.add(train_data)\n",
        "\n",
        "    results = []\n",
        "    for ef in efSearch_vals:\n",
        "        index.hnsw.efSearch = ef\n",
        "        start_time = time.time()\n",
        "        _, indices = index.search(test_queries, 1)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        recall = np.mean(indices[:, 0] == ground_truth)\n",
        "        qps = len(test_queries) / elapsed_time\n",
        "        results.append((ef, recall, qps))\n",
        "\n",
        "    print(\"HNSW evaluated.\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_scann(train_data, test_queries, ground_truth, num_neighbors=1, num_search_trees=30, num_leaves=2000, num_leaves_to_search_vals=[10, 50, 100, 200], quantize=True):\n",
        "    train_data = train_data.astype(np.float32)\n",
        "    test_queries = test_queries.astype(np.float32)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for num_leaves_to_search in num_leaves_to_search_vals:\n",
        "        index = scann.scann_ops_pybind.builder(train_data, num_neighbors, \"dot_product\") \\\n",
        "        .tree(num_leaves=num_leaves, num_leaves_to_search=num_leaves_to_search).score_ah(2, anisotropic_quantization_threshold=0.2).build()\n",
        "\n",
        "        start_time = time.time()\n",
        "        _, indices = index.search(test_queries, num_neighbors)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        recall = np.mean(indices[:, 0] == ground_truth)\n",
        "        qps = len(test_queries) / elapsed_time\n",
        "\n",
        "        results.append((num_leaves_to_search, recall, qps))\n",
        "\n",
        "    print(\"ScaNN evaluated.\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "PYj3zK93nQ90"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0eQc3IuVPPFW"
      },
      "outputs": [],
      "source": [
        "def evaluate_lsh(train_data, test_queries, ground_truth, nbits_vals=[32, 64, 512, 768]):\n",
        "    d = train_data.shape[1]\n",
        "    results = []\n",
        "\n",
        "    for nbits in nbits_vals:\n",
        "        index = faiss.IndexLSH(d, nbits)\n",
        "        index.train(train_data)\n",
        "        index.add(train_data)\n",
        "\n",
        "        start_time = time.time()\n",
        "        _, indices = index.search(test_queries, 1)\n",
        "        elapsed_time = time.time() - start_time\n",
        "\n",
        "        recall = np.mean(indices[:, 0] == ground_truth)\n",
        "        qps = len(test_queries) / elapsed_time\n",
        "        results.append((nbits, recall, qps))\n",
        "\n",
        "    print(\"LSH evaluated.\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lusnj10jPPFW"
      },
      "outputs": [],
      "source": [
        "def plot_results(hnsw_results, lsh_results):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    for ef, recall, qps in hnsw_results:\n",
        "        plt.scatter(qps, recall, label=f'HNSW ef={ef}', marker='o')\n",
        "    for nbits, recall, qps in lsh_results:\n",
        "        plt.scatter(qps, recall, label=f'LSH nbits={nbits}', marker='x')\n",
        "\n",
        "    plt.xlabel(\"Queries Per Second (QPS)\")\n",
        "    plt.ylabel(\"1-Recall@1\")\n",
        "    plt.title(\"HNSW vs LSH: QPS vs Recall\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_sift1m()\n",
        "train_data, test_queries, ground_truth = load_sift1m()\n",
        "hnsw_results = evaluate_hnsw(train_data, test_queries, ground_truth)\n",
        "scann_results = evaluate_scann(train_data, test_queries, ground_truth)\n",
        "plot_results(hnsw_results, scann_results)"
      ],
      "metadata": {
        "id": "wYfyY2zyUev3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e2940aed-7e3d-43d7-b902-0dcd50534512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading SIFT1M dataset...\n",
            "Download complete.\n",
            "Dataset loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scann_results = evaluate_scann(train_data, test_queries, ground_truth)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "rv94lDT6aGJ0",
        "outputId": "8e7cb43c-cd19-4b0f-d9d4-3cd5c33ad8b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Query must be one-dimensional",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-c3e271f966e2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscann_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_scann\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_queries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-9bf7d1f364d9>\u001b[0m in \u001b[0;36mevaluate_scann\u001b[0;34m(train_data, test_queries, ground_truth, num_neighbors, num_search_trees, num_leaves, num_leaves_to_search_vals, quantize)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Perform search for each query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_queries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scann/scann_ops/py/scann_ops_pybind.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, q, final_num_neighbors, pre_reorder_num_neighbors, leaves_to_search)\u001b[0m\n\u001b[1;32m     39\u001b[0m   ):\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m\"\"\"Single-query search; -1 for a param uses the searcher's default value.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     idx, dist = self.searcher.search(q, final_num_neighbors,\n\u001b[0m\u001b[1;32m     42\u001b[0m                                      \u001b[0mpre_reorder_num_neighbors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                                      leaves_to_search)\n",
            "\u001b[0;31mValueError\u001b[0m: Query must be one-dimensional"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}